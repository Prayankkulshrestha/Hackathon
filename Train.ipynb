{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split,StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler,KBinsDiscretizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm.notebook import tqdm ,tnrange\n",
    "\n",
    "pd. set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(r'../input/train.csv')\n",
    "test_data = pd.read_csv(r'../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(245725, 11)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Duplicate based on ID\n",
    "data=train_data.drop_duplicates(subset='ID', keep=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sales variable: Combine the Region code and Channel code \n",
    "2. Customer Demographics : Combine the Occupation and gender\n",
    "3. Extract Vintage year and Month from Vintage column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales channel : based on Channel code & Region code\n",
    "train_data['Region_channel'] = train_data['Channel_Code'].astype(str)+'_'+train_data['Region_Code'].astype(str)\n",
    "test_data['Region_channel'] = test_data['Channel_Code'].astype(str)+'_'+test_data['Region_Code'].astype(str)\n",
    "\n",
    "# Demographics: based on Occupation & Gender\n",
    "train_data['Demographics'] = train_data['Occupation'].astype(str)+'_'+train_data['Gender'].astype(str)\n",
    "test_data['Demographics'] = test_data['Occupation'].astype(str)+'_'+test_data['Gender'].astype(str)\n",
    "\n",
    "# Convert vintage into years as float\n",
    "train_data['Vintage'] = train_data['Vintage']/12\n",
    "test_data['Vintage'] = test_data['Vintage']/12\n",
    "\n",
    "train_data['Vintage'] = train_data['Vintage'].astype(str)\n",
    "test_data['Vintage'] = test_data['Vintage'].astype(str)\n",
    "\n",
    "# Extract Vintage month and years\n",
    "train_data['Vintage_year'] = train_data['Vintage'].apply(lambda x: x.split(\".\")[0])\n",
    "train_data['Vintage_month'] = train_data['Vintage'].apply(lambda x: x.split(\".\")[1][0])\n",
    "test_data['Vintage_year'] = test_data['Vintage'].apply(lambda x: x.split(\".\")[0])\n",
    "test_data['Vintage_month'] = test_data['Vintage'].apply(lambda x: x.split(\".\")[1][0])\n",
    "\n",
    "# Drop the Vintage\n",
    "train_data.drop('Vintage',axis=1,inplace=True)\n",
    "test_data.drop('Vintage',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature types\n",
    "cat_features = ['Gender','Region_Code','Occupation','Channel_Code',\n",
    "                'Vintage_year','Credit_Product','Is_Active','Region_channel','Demographics','Vintage_month']\n",
    "cont_features = ['Age','Avg_Account_Balance']\n",
    "target = 'Is_Lead'\n",
    "\n",
    "def encoder(data,label_features):\n",
    "    for feat in label_features:\n",
    "        encoder = LabelEncoder()\n",
    "        col = data[feat].fillna(\"UNKNOWN\").astype(str).values\n",
    "        data.loc[:, feat] = encoder.fit_transform(col)    \n",
    "    return data\n",
    "\n",
    " # Label Encoding the variable   \n",
    "train_data = encoder(train_data,cat_features)\n",
    "test_data = encoder(test_data,cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the data for further Feature Engineering\n",
    "train_data['Istrain'] = 1\n",
    "test_data['Istrain'] = 0\n",
    "\n",
    "# Combine the data\n",
    "combined_data = pd.concat([train_data,test_data],axis =0).reset_index(drop = True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The discretization transform is available in the scikit-learn Python machine learning library via the KBinsDiscretizer class.**\n",
    "\n",
    "**Uniform: Each bin has the same width in the span of possible values for the variable.**\n",
    "\n",
    "**Quantile: Each bin has the same number of values, split based on percentiles.**\n",
    "\n",
    "**Clustered: Clusters are identified and examples are assigned to each group**\n",
    "\n",
    "Both Continous variables are skewed use Quantile based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bining the Average account balance & Age\n",
    "for cont_feat in cont_features:\n",
    "    discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\n",
    "    feature_name = cont_feat+\"_\"+\"bins\"\n",
    "    combined_data[feature_name] = discretizer.fit_transform(combined_data[cont_feat].values.reshape(-1,1)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['Gender_Counts'] = combined_data['Gender'].map(combined_data['Gender'].value_counts().to_dict())\n",
    "combined_data['Region_counts'] = combined_data['Region_Code'].map(combined_data['Region_Code'].value_counts().to_dict())\n",
    "combined_data['Channel_Code_Counts'] = combined_data['Channel_Code'].map(combined_data['Channel_Code'].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Agg based on different level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Account amount based on region channel\n",
    "combined_data['mean_balance_sales']=combined_data.groupby(['Region_channel'])['Avg_Account_Balance'].transform('mean')\n",
    "combined_data['std_balance_sales']=combined_data.groupby(['Region_channel'])['Avg_Account_Balance'].transform('std')\n",
    "combined_data['min_balance_sales']=combined_data.groupby(['Region_channel'])['Avg_Account_Balance'].transform('min')\n",
    "combined_data['max_balance_sales']=combined_data.groupby(['Region_channel'])['Avg_Account_Balance'].transform('max')\n",
    "\n",
    "#Age based on Region channel\n",
    "combined_data['mean_age_sales']=combined_data.groupby(['Region_channel'])['Age'].transform('mean')\n",
    "combined_data['std_age_sales']=combined_data.groupby(['Region_channel'])['Age'].transform('std')\n",
    "combined_data['min_age_sales']=combined_data.groupby(['Region_channel'])['Age'].transform('min')\n",
    "combined_data['max_age_sales']=combined_data.groupby(['Region_channel'])['Age'].transform('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Account amount based on Customer Demographics\n",
    "combined_data['mean_balance_demo']=combined_data.groupby(['Demographics'])['Avg_Account_Balance'].transform('mean')\n",
    "combined_data['std_balance_demo']=combined_data.groupby(['Demographics'])['Avg_Account_Balance'].transform('std')\n",
    "combined_data['min_balance_demo']=combined_data.groupby(['Demographics'])['Avg_Account_Balance'].transform('min')\n",
    "combined_data['max_balance_demo']=combined_data.groupby(['Demographics'])['Avg_Account_Balance'].transform('max')\n",
    "\n",
    "#Age based on Customer Demographics\n",
    "combined_data['mean_age_demo']=combined_data.groupby(['Demographics'])['Age'].transform('mean')\n",
    "combined_data['std_age_demo']=combined_data.groupby(['Demographics'])['Age'].transform('std')\n",
    "combined_data['min_age_demo']=combined_data.groupby(['Demographics'])['Age'].transform('min')\n",
    "combined_data['max_age_demo']=combined_data.groupby(['Demographics'])['Age'].transform('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature Based Credit product and occupation\n",
    "combined_data['mean_balance_credit_occ']=combined_data.groupby(['Credit_Product','Occupation'])['Avg_Account_Balance'].transform('mean')\n",
    "combined_data['std_balance_credit_occ']=combined_data.groupby(['Credit_Product','Occupation'])['Avg_Account_Balance'].transform('std')\n",
    "combined_data['min_balance_credit_occ']=combined_data.groupby(['Credit_Product','Occupation'])['Avg_Account_Balance'].transform('min')\n",
    "combined_data['max_balance_credit_occ']=combined_data.groupby(['Credit_Product','Occupation'])['Avg_Account_Balance'].transform('max')\n",
    "\n",
    "\n",
    "combined_data['mean_age_credit_occ']=combined_data.groupby(['Credit_Product','Occupation'])['Age'].transform('mean')\n",
    "combined_data['std_age_credit_occ']=combined_data.groupby(['Credit_Product','Occupation'])['Age'].transform('std')\n",
    "combined_data['min_age_credit_occ']=combined_data.groupby(['Credit_Product','Occupation'])['Age'].transform('min')\n",
    "combined_data['max_age_credit_occ']=combined_data.groupby(['Credit_Product','Occupation'])['Age'].transform('max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Based on Active account and Occupation\n",
    "\n",
    "combined_data['mean_balance_credit_active']=combined_data.groupby(['Is_Active','Occupation'])['Avg_Account_Balance'].transform('mean')\n",
    "combined_data['std_balance_credit_active']=combined_data.groupby(['Is_Active','Occupation'])['Avg_Account_Balance'].transform('std')\n",
    "combined_data['min_balance_credit_active']=combined_data.groupby(['Is_Active','Occupation'])['Avg_Account_Balance'].transform('min')\n",
    "combined_data['max_balance_credit_active']=combined_data.groupby(['Is_Active','Occupation'])['Avg_Account_Balance'].transform('max')\n",
    "\n",
    "combined_data['mean_age_credit_active']=combined_data.groupby(['Is_Active','Occupation'])['Age'].transform('mean')\n",
    "combined_data['std_age_credit_active']=combined_data.groupby(['Is_Active','Occupation'])['Age'].transform('std')\n",
    "combined_data['min_age_credit_active']=combined_data.groupby(['Is_Active','Occupation'])['Age'].transform('min')\n",
    "combined_data['max_age_credit_active']=combined_data.groupby(['Is_Active','Occupation'])['Age'].transform('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature based on Credit product and customer Demographics\n",
    "\n",
    "combined_data['mean_balance_credit_demo']=combined_data.groupby(['Credit_Product','Demographics'])['Avg_Account_Balance'].transform('mean')\n",
    "combined_data['std_balance_credit_demo']=combined_data.groupby(['Credit_Product','Demographics'])['Avg_Account_Balance'].transform('std')\n",
    "combined_data['min_balance_credit_demo']=combined_data.groupby(['Credit_Product','Demographics'])['Avg_Account_Balance'].transform('min')\n",
    "combined_data['max_balance_credit_demo']=combined_data.groupby(['Credit_Product','Demographics'])['Avg_Account_Balance'].transform('max')\n",
    "\n",
    "combined_data['mean_balance_active_demo']=combined_data.groupby(['Is_Active','Demographics'])['Avg_Account_Balance'].transform('mean')\n",
    "combined_data['std_balance_active_demo']=combined_data.groupby(['Is_Active','Demographics'])['Avg_Account_Balance'].transform('std')\n",
    "combined_data['min_balance_active_demo']=combined_data.groupby(['Is_Active','Demographics'])['Avg_Account_Balance'].transform('min')\n",
    "combined_data['max_balance_active_demo']=combined_data.groupby(['Is_Active','Demographics'])['Avg_Account_Balance'].transform('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature based on Credit product and customer demographics\n",
    "\n",
    "combined_data['mean_age_credit_demo']=combined_data.groupby(['Credit_Product','Demographics'])['Age'].transform('mean')\n",
    "combined_data['std_age_credit_demo']=combined_data.groupby(['Credit_Product','Demographics'])['Age'].transform('std')\n",
    "combined_data['min_age_credit_demo']=combined_data.groupby(['Credit_Product','Demographics'])['Age'].transform('min')\n",
    "combined_data['max_age_credit_demo']=combined_data.groupby(['Credit_Product','Demographics'])['Age'].transform('max')\n",
    "\n",
    "combined_data['mean_age_active_demo']=combined_data.groupby(['Is_Active','Demographics'])['Age'].transform('mean')\n",
    "combined_data['std_age_active_demo']=combined_data.groupby(['Is_Active','Demographics'])['Age'].transform('std')\n",
    "combined_data['min_age_active_demo']=combined_data.groupby(['Is_Active','Demographics'])['Age'].transform('min')\n",
    "combined_data['max_age_active_demo']=combined_data.groupby(['Is_Active','Demographics'])['Age'].transform('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature based on both sales channel demographics based\n",
    "combined_data['mean_credit_Region_channel_demo'] = combined_data.groupby(['Region_channel','Demographics'])['Avg_Account_Balance'].transform('mean')\n",
    "combined_data['max_credit_Region_channel_demo'] = combined_data.groupby(['Region_channel','Demographics'])['Avg_Account_Balance'].transform('max')\n",
    "combined_data['min_credit_Region_channel_demo'] = combined_data.groupby(['Region_channel','Demographics'])['Avg_Account_Balance'].transform('min')\n",
    "\n",
    "combined_data['mean_age_Region_channel_demo'] = combined_data.groupby(['Region_channel','Demographics'])['Age'].transform('mean')\n",
    "combined_data['max_age_Region_channel_demo'] = combined_data.groupby(['Region_channel','Demographics'])['Age'].transform('max')\n",
    "combined_data['min_age_Region_channel_demo'] = combined_data.groupby(['Region_channel','Demographics'])['Age'].transform('min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_feature = ['ID','Gender','Region_Code','Occupation','Channel_Code',\n",
    "                'Credit_Product','Is_Active','Is_Lead','Region_channel',\n",
    "                'Demographics','Vintage_year','Vintage_month','Istrain','Avg_Account_Balance_bins','Age_Bins']\n",
    "\n",
    "cont_features = [x for x in combined_data.columns if x not in label_feature]\n",
    "\n",
    "def std_encoder(data,std_feat):\n",
    "    for feat in std_feat:\n",
    "        std = StandardScaler()\n",
    "        temp_col = data[feat].fillna(data[feat].mean()).astype(int).values\n",
    "        data.loc[:, feat] = std.fit_transform(temp_col.reshape(-1,1))        \n",
    "    return data\n",
    "\n",
    "final_data = std_encoder(combined_data,cont_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = final_data[final_data['Istrain']==1]\n",
    "test_df = final_data[final_data['Istrain']==0]\n",
    "\n",
    "target = train_df['Is_Lead']\n",
    "train_df = train_df.drop(columns =['Istrain','ID','Is_Lead'],axis=1)\n",
    "test_df = test_df.drop(columns=['Istrain','ID','Is_Lead'],axis=1)\n",
    "X_train, y_train, X_test = train_df.copy(), target.copy(),test_df.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Approch\n",
    "1. Start with Randomforest base line ---> 0.74 ROC score (Not included in this work)\n",
    "2. LGBM -- > 0.785 (not inlcude in this work)\n",
    "\n",
    "*Final Model :weighted voted classifier of LGBM & XGBoost and catboost*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Final Params\n",
    "\n",
    "## Hyperparameter is mix of both previous work of people and run & trails\n",
    "\n",
    "cat_features = [x for x in train_df.columns if x not in cont_features]\n",
    "\n",
    "model_dispatcher = {\"lgbm\" : lgb.LGBMClassifier(boosting_type='gbdt',\n",
    "                                 n_estimators=15000,\n",
    "                                 max_depth=12,\n",
    "                                 learning_rate=0.02,\n",
    "                                 subsample=0.9,\n",
    "                                 colsample_bytree=0.4,\n",
    "                                 objective ='binary',\n",
    "                                 random_state = 27,\n",
    "                                 importance_type='gain',\n",
    "                                 reg_alpha=2,\n",
    "                                 reg_lambda=2),\n",
    "                    \n",
    "                'xgboost' :   xgb.XGBClassifier(n_estimators=1200,\n",
    "                                max_depth=8,\n",
    "                                learning_rate=0.04,\n",
    "                                subsample=0.9,\n",
    "                                colsample_bytree=0.4,\n",
    "                                objective = 'binary:logistic',\n",
    "                                random_state = 27\n",
    "                               ),\n",
    "                    \n",
    "                \"Catboost\" : CatBoostClassifier(iterations=15000,\n",
    "                                learning_rate=0.02,\n",
    "                                random_strength=0.1,\n",
    "                                depth=12,\n",
    "                                loss_function='Logloss',\n",
    "                                eval_metric='Logloss',\n",
    "                                leaf_estimation_method='Newton',\n",
    "                                random_state = 27,\n",
    "                                cat_features =cat_features,\n",
    "                                subsample = 0.9,\n",
    "                                rsm = 0.8\n",
    "                                )}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Fold 0:\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.35499\tvalid_0's auc: 0.871979\n",
      "[200]\tvalid_0's binary_logloss: 0.345139\tvalid_0's auc: 0.87358\n",
      "[300]\tvalid_0's binary_logloss: 0.343979\tvalid_0's auc: 0.87416\n",
      "[400]\tvalid_0's binary_logloss: 0.343631\tvalid_0's auc: 0.874328\n",
      "[500]\tvalid_0's binary_logloss: 0.343523\tvalid_0's auc: 0.8744\n",
      "Early stopping, best iteration is:\n",
      "[499]\tvalid_0's binary_logloss: 0.343523\tvalid_0's auc: 0.8744\n",
      "Predicting ....\n",
      "Scoring the model ....\n",
      "\n",
      "\n",
      " validation ROC is 0.874400135428131:\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "Training Fold 1:\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.354819\tvalid_0's auc: 0.871619\n",
      "[200]\tvalid_0's binary_logloss: 0.345242\tvalid_0's auc: 0.873161\n",
      "[300]\tvalid_0's binary_logloss: 0.344114\tvalid_0's auc: 0.873527\n",
      "[400]\tvalid_0's binary_logloss: 0.3438\tvalid_0's auc: 0.873546\n",
      "Early stopping, best iteration is:\n",
      "[366]\tvalid_0's binary_logloss: 0.343837\tvalid_0's auc: 0.873642\n",
      "Predicting ....\n",
      "Scoring the model ....\n",
      "\n",
      "\n",
      " validation ROC is 0.8736416471723562:\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "Training Fold 2:\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.355518\tvalid_0's auc: 0.871619\n",
      "[200]\tvalid_0's binary_logloss: 0.345912\tvalid_0's auc: 0.872852\n",
      "[300]\tvalid_0's binary_logloss: 0.344764\tvalid_0's auc: 0.87336\n",
      "[400]\tvalid_0's binary_logloss: 0.344468\tvalid_0's auc: 0.873513\n",
      "[500]\tvalid_0's binary_logloss: 0.344471\tvalid_0's auc: 0.873518\n",
      "Early stopping, best iteration is:\n",
      "[472]\tvalid_0's binary_logloss: 0.34446\tvalid_0's auc: 0.873533\n",
      "Predicting ....\n",
      "Scoring the model ....\n",
      "\n",
      "\n",
      " validation ROC is 0.8735330093820659:\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "Training Fold 3:\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.356166\tvalid_0's auc: 0.870388\n",
      "[200]\tvalid_0's binary_logloss: 0.346687\tvalid_0's auc: 0.87172\n",
      "[300]\tvalid_0's binary_logloss: 0.345601\tvalid_0's auc: 0.872348\n",
      "[400]\tvalid_0's binary_logloss: 0.345239\tvalid_0's auc: 0.87266\n",
      "[500]\tvalid_0's binary_logloss: 0.345205\tvalid_0's auc: 0.872659\n",
      "Early stopping, best iteration is:\n",
      "[472]\tvalid_0's binary_logloss: 0.345177\tvalid_0's auc: 0.872712\n",
      "Predicting ....\n",
      "Scoring the model ....\n",
      "\n",
      "\n",
      " validation ROC is 0.872712184891678:\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "Training Fold 4:\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.353301\tvalid_0's auc: 0.873265\n",
      "[200]\tvalid_0's binary_logloss: 0.343364\tvalid_0's auc: 0.87486\n",
      "[300]\tvalid_0's binary_logloss: 0.342239\tvalid_0's auc: 0.875455\n",
      "[400]\tvalid_0's binary_logloss: 0.342059\tvalid_0's auc: 0.875583\n",
      "Early stopping, best iteration is:\n",
      "[386]\tvalid_0's binary_logloss: 0.342041\tvalid_0's auc: 0.875616\n",
      "Predicting ....\n",
      "Scoring the model ....\n",
      "\n",
      "\n",
      " validation ROC is 0.8756158671838221:\n",
      "********************************************************************************\n",
      "Evalution metrics statatics\n",
      "\n",
      "Log Loss : 0.3438074179,0.0010490983\n",
      "0.8739805688 (0.0009771604)\n",
      "Wall time: 56.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "probs_score_lgb = np.zeros(shape=(test_df.shape[0],))\n",
    "scores,avg_loss = [],[]\n",
    "sskf = StratifiedShuffleSplit(n_splits=5, test_size = 0.34 ,random_state=27)\n",
    "\n",
    "for fold_, (train_idx, val_idx) in enumerate(sskf.split(X_train,y_train)):\n",
    "    print(\"\\n\")\n",
    "    print('Training Fold {}:'.format(fold_))\n",
    "    \n",
    "    xtrain = X_train.iloc[train_idx]\n",
    "    ytrain = y_train.iloc[train_idx]\n",
    "    \n",
    "    xval = X_train.iloc[val_idx]\n",
    "    yval = y_train.iloc[val_idx]\n",
    "    \n",
    "    model = model_dispatcher['lgbm']\n",
    "    \n",
    "    \n",
    "    classsifier = model.fit(xtrain, ytrain,\n",
    "                           eval_set = [(xval, yval)],\n",
    "                           verbose = 100,\n",
    "                           eval_metric = ['binary_logloss','auc'],\n",
    "                           early_stopping_rounds=100)\n",
    "    \n",
    "    # predicting \n",
    "    print(\"Predicting ....\")\n",
    "    preds = classsifier.predict_proba(xval)[:,1]\n",
    "    probs_score_lgb += classsifier.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    print(\"Scoring the model ....\")\n",
    "    roc_score = roc_auc_score(yval,preds)\n",
    "    \n",
    "    # appending the metrics\n",
    "    scores.append(roc_score)\n",
    "    avg_loss.append(classsifier.best_score_['valid_0']['binary_logloss'])\n",
    "    \n",
    "    print ('\\n\\n validation ROC is {}:'.format(roc_score))\n",
    "    print('*'*80)\n",
    "    \n",
    "print(\"Evalution metrics statatics\\n\")   \n",
    "print(\"Log Loss : {0:.10f},{1:.10f}\".format(np.array(avg_loss).mean(), np.array(avg_loss).std()))\n",
    "print('%.10f (%.10f)' % (np.array(scores).mean(), np.array(scores).std()))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Fold 0:\n",
      "[0]\tvalidation_0-logloss:0.67036\tvalidation_0-auc:0.87064\n",
      "[100]\tvalidation_0-logloss:0.34543\tvalidation_0-auc:0.87338\n",
      "[200]\tvalidation_0-logloss:0.34448\tvalidation_0-auc:0.87363\n",
      "[247]\tvalidation_0-logloss:0.34477\tvalidation_0-auc:0.87339\n",
      "Predicting ....\n",
      "Scoring the model ....\n",
      "\n",
      "\n",
      " validation ROC is 0.8736265297674767:\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "Training Fold 1:\n",
      "[0]\tvalidation_0-logloss:0.67035\tvalidation_0-auc:0.86998\n",
      "[100]\tvalidation_0-logloss:0.34550\tvalidation_0-auc:0.87267\n",
      "[200]\tvalidation_0-logloss:0.34477\tvalidation_0-auc:0.87268\n",
      "[223]\tvalidation_0-logloss:0.34490\tvalidation_0-auc:0.87253\n",
      "Predicting ....\n",
      "Scoring the model ....\n",
      "\n",
      "\n",
      " validation ROC is 0.8728866371944652:\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "Training Fold 2:\n",
      "[0]\tvalidation_0-logloss:0.67041\tvalidation_0-auc:0.86899\n",
      "[100]\tvalidation_0-logloss:0.34642\tvalidation_0-auc:0.87223\n",
      "[200]\tvalidation_0-logloss:0.34579\tvalidation_0-auc:0.87238\n",
      "[249]\tvalidation_0-logloss:0.34603\tvalidation_0-auc:0.87219\n",
      "Predicting ....\n",
      "Scoring the model ....\n",
      "\n",
      "\n",
      " validation ROC is 0.8724622480996596:\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "Training Fold 3:\n",
      "[0]\tvalidation_0-logloss:0.67041\tvalidation_0-auc:0.86838\n",
      "[100]\tvalidation_0-logloss:0.34716\tvalidation_0-auc:0.87133\n",
      "[200]\tvalidation_0-logloss:0.34624\tvalidation_0-auc:0.87156\n",
      "[253]\tvalidation_0-logloss:0.34658\tvalidation_0-auc:0.87131\n",
      "Predicting ....\n",
      "Scoring the model ....\n",
      "\n",
      "\n",
      " validation ROC is 0.871622158082873:\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "Training Fold 4:\n",
      "[0]\tvalidation_0-logloss:0.67033\tvalidation_0-auc:0.87034\n",
      "[100]\tvalidation_0-logloss:0.34376\tvalidation_0-auc:0.87449\n",
      "[200]\tvalidation_0-logloss:0.34316\tvalidation_0-auc:0.87458\n",
      "[252]\tvalidation_0-logloss:0.34353\tvalidation_0-auc:0.87420\n",
      "Predicting ....\n",
      "Scoring the model ....\n",
      "\n",
      "\n",
      " validation ROC is 0.8745850055413568:\n",
      "********************************************************************************\n",
      "Evalution metrics statatics\n",
      "\n",
      "Log Loss : 0.8730862000,0.0010112955\n",
      "0.8730365157 (0.0010098110)\n",
      "Wall time: 3min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "probs_score_xgboost = np.zeros(shape=(test_df.shape[0],))\n",
    "scores,avg_loss = [],[]\n",
    "sskf = StratifiedShuffleSplit(n_splits=5, test_size = 0.34 ,random_state=27)\n",
    "\n",
    "for fold_, (train_idx, val_idx) in enumerate(sskf.split(X_train,y_train)):\n",
    "    print(\"\\n\")\n",
    "    print('Training Fold {}:'.format(fold_))\n",
    "    \n",
    "    xtrain = X_train.iloc[train_idx]\n",
    "    ytrain = y_train.iloc[train_idx]\n",
    "    \n",
    "    xval = X_train.iloc[val_idx]\n",
    "    yval = y_train.iloc[val_idx]\n",
    "    \n",
    "    model = model_dispatcher['xgboost']\n",
    "    \n",
    "    \n",
    "    classsifier = model.fit(xtrain, ytrain,\n",
    "                           eval_set = [(xval, yval)],\n",
    "                           verbose = 100,\n",
    "                           eval_metric = ['logloss','auc'],\n",
    "                           early_stopping_rounds=100)\n",
    "    \n",
    "    # predicting \n",
    "    print(\"Predicting ....\")\n",
    "    preds = classsifier.predict_proba(xval)[:,1]\n",
    "    probs_score_xgboost += classsifier.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    print(\"Scoring the model ....\")\n",
    "    roc_score = roc_auc_score(yval,preds)\n",
    "    \n",
    "    # appending the metrics\n",
    "    scores.append(roc_score)\n",
    "    avg_loss.append(classsifier.best_score)\n",
    "    \n",
    "    print ('\\n\\n validation ROC is {}:'.format(roc_score))\n",
    "    print('*'*80)\n",
    "    \n",
    "print(\"Evalution metrics statatics\\n\")   \n",
    "print(\"Log Loss : {0:.10f},{1:.10f}\".format(np.array(avg_loss).mean(), np.array(avg_loss).std()))\n",
    "print('%.10f (%.10f)' % (np.array(scores).mean(), np.array(scores).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Fold 0:\n",
      "0:\tlearn: 0.6752997\ttest: 0.6753023\tbest: 0.6753023 (0)\ttotal: 816ms\tremaining: 3h 24m\n",
      "100:\tlearn: 0.3374270\ttest: 0.3479003\tbest: 0.3479003 (100)\ttotal: 1m 34s\tremaining: 3h 51m 20s\n",
      "200:\tlearn: 0.3237355\ttest: 0.3444605\tbest: 0.3444565 (195)\ttotal: 3m 15s\tremaining: 3h 59m 36s\n",
      "300:\tlearn: 0.3177089\ttest: 0.3443930\tbest: 0.3443662 (254)\ttotal: 4m 58s\tremaining: 4h 2m 42s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.3443661566\n",
      "bestIteration = 254\n",
      "\n",
      "Shrink model to first 255 iterations.\n",
      "Predicting ....\n",
      "Scoring the model ....\n",
      "\n",
      "\n",
      " validation ROC is 0.8736806984228702:\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "Training Fold 1:\n",
      "0:\tlearn: 0.6753250\ttest: 0.6753207\tbest: 0.6753207 (0)\ttotal: 1.02s\tremaining: 4h 14m 45s\n",
      "100:\tlearn: 0.3373814\ttest: 0.3482061\tbest: 0.3482061 (100)\ttotal: 1m 39s\tremaining: 4h 3m 58s\n",
      "200:\tlearn: 0.3230216\ttest: 0.3446013\tbest: 0.3445953 (199)\ttotal: 3m 23s\tremaining: 4h 9m 17s\n",
      "300:\tlearn: 0.3161561\ttest: 0.3444609\tbest: 0.3444509 (287)\ttotal: 5m 8s\tremaining: 4h 10m 47s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.3444509283\n",
      "bestIteration = 287\n",
      "\n",
      "Shrink model to first 288 iterations.\n",
      "Predicting ....\n",
      "Scoring the model ....\n",
      "\n",
      "\n",
      " validation ROC is 0.8729784307134829:\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "Training Fold 2:\n",
      "0:\tlearn: 0.6751832\ttest: 0.6753012\tbest: 0.6753012 (0)\ttotal: 863ms\tremaining: 3h 35m 49s\n",
      "100:\tlearn: 0.3368175\ttest: 0.3486812\tbest: 0.3486812 (100)\ttotal: 1m 39s\tremaining: 4h 5m 10s\n",
      "200:\tlearn: 0.3235565\ttest: 0.3452683\tbest: 0.3452683 (200)\ttotal: 3m 23s\tremaining: 4h 9m 19s\n",
      "300:\tlearn: 0.3167899\ttest: 0.3452766\tbest: 0.3451839 (262)\ttotal: 5m 4s\tremaining: 4h 7m 44s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.3451838727\n",
      "bestIteration = 262\n",
      "\n",
      "Shrink model to first 263 iterations.\n",
      "Predicting ....\n",
      "Scoring the model ....\n",
      "\n",
      "\n",
      " validation ROC is 0.8731385593681804:\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "Training Fold 3:\n",
      "0:\tlearn: 0.6752123\ttest: 0.6753165\tbest: 0.6753165 (0)\ttotal: 833ms\tremaining: 3h 28m 15s\n",
      "100:\tlearn: 0.3368997\ttest: 0.3495863\tbest: 0.3495863 (100)\ttotal: 1m 43s\tremaining: 4h 14m 46s\n",
      "200:\tlearn: 0.3231787\ttest: 0.3461594\tbest: 0.3461594 (200)\ttotal: 3m 32s\tremaining: 4h 21m 13s\n",
      "300:\tlearn: 0.3175831\ttest: 0.3459972\tbest: 0.3459970 (298)\ttotal: 5m 10s\tremaining: 4h 12m 50s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.345986242\n",
      "bestIteration = 321\n",
      "\n",
      "Shrink model to first 322 iterations.\n",
      "Predicting ....\n",
      "Scoring the model ....\n",
      "\n",
      "\n",
      " validation ROC is 0.8722724543190038:\n",
      "********************************************************************************\n",
      "\n",
      "\n",
      "Training Fold 4:\n",
      "0:\tlearn: 0.6754131\ttest: 0.6754783\tbest: 0.6754783 (0)\ttotal: 901ms\tremaining: 3h 45m 11s\n",
      "100:\tlearn: 0.3389079\ttest: 0.3472004\tbest: 0.3472004 (100)\ttotal: 1m 38s\tremaining: 4h 1m 3s\n",
      "200:\tlearn: 0.3244564\ttest: 0.3432838\tbest: 0.3432838 (200)\ttotal: 3m 21s\tremaining: 4h 7m 49s\n",
      "300:\tlearn: 0.3169801\ttest: 0.3432730\tbest: 0.3432231 (260)\ttotal: 5m 7s\tremaining: 4h 10m 1s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.3432231035\n",
      "bestIteration = 260\n",
      "\n",
      "Shrink model to first 261 iterations.\n",
      "Predicting ....\n",
      "Scoring the model ....\n",
      "\n",
      "\n",
      " validation ROC is 0.8746598724442578:\n",
      "********************************************************************************\n",
      "Evalution metrics statatics\n",
      "\n",
      "Log Loss : 0.3446420606,0.0009193996\n",
      "0.8733460031 (0.0007961059)\n",
      "Wall time: 27min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "probs_score_catboost = np.zeros(shape=(test_df.shape[0],))\n",
    "scores,avg_loss = [],[]\n",
    "sskf = StratifiedShuffleSplit(n_splits=5, test_size = 0.34 ,random_state=27)\n",
    "\n",
    "for fold_, (train_idx, val_idx) in enumerate(sskf.split(X_train,y_train)):\n",
    "    print(\"\\n\")\n",
    "    print('Training Fold {}:'.format(fold_))\n",
    "    \n",
    "    xtrain = X_train.iloc[train_idx]\n",
    "    ytrain = y_train.iloc[train_idx]\n",
    "    \n",
    "    xval = X_train.iloc[val_idx]\n",
    "    yval = y_train.iloc[val_idx]\n",
    "    \n",
    "    model = model_dispatcher['Catboost']\n",
    "    \n",
    "    \n",
    "    classsifier = model.fit(xtrain, ytrain,\n",
    "                           eval_set = [(xval, yval)],\n",
    "                           verbose = 100,\n",
    "                           early_stopping_rounds=50)\n",
    "    \n",
    "    # predicting \n",
    "    print(\"Predicting ....\")\n",
    "    preds = classsifier.predict_proba(xval)[:,1]\n",
    "    probs_score_catboost += classsifier.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    print(\"Scoring the model ....\")\n",
    "    roc_score = roc_auc_score(yval,preds)\n",
    "    \n",
    "    # appending the metrics\n",
    "    scores.append(roc_score)\n",
    "    avg_loss.append(classsifier.best_score_['validation']['Logloss'])\n",
    "    \n",
    "    print ('\\n\\n validation ROC is {}:'.format(roc_score))\n",
    "    print('*'*80)\n",
    "    \n",
    "print(\"Evalution metrics statatics\\n\")   \n",
    "print(\"Log Loss : {0:.10f},{1:.10f}\".format(np.array(avg_loss).mean(), np.array(avg_loss).std()))\n",
    "print('%.10f (%.10f)' % (np.array(scores).mean(), np.array(scores).std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### weighted Avearge Voting classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LGBM\n",
    "p1 = probs_score_lgb/5\n",
    "\n",
    "# Catboost\n",
    "p2 = probs_score_catboost/5\n",
    "\n",
    "# Xgboost\n",
    "p3 = probs_score_xgboost/5\n",
    "\n",
    "submission = pd.read_csv('../input/submission.csv')\n",
    "\n",
    "submission['Is_Lead'] = 0.15*p1 + 0.7*p2 + 0.15*p3\n",
    "\n",
    "submission.to_csv('../input/subsmiison.csv',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
